{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "For this project. We will train an agent to navigate and collect bananas in a square world.\n",
    "\n",
    "\n",
    "A reward of +1 is provided for collecting a yellow banana, and a reward of -1 is provided for collecting a blue banana. The goal of your agent is to collect as many yellow bananas as possible while avoiding blue bananas.\n",
    "\n",
    "The state space has 37 dimensions and contains the agent's velocity. Along with ray-based perception of objects around the agent's forward direction. Due to this information, the agent has to learn how to best select actions. Four discrete actions are available:\n",
    "\n",
    "0 - move forward.\n",
    "1 - move backward.\n",
    "2 - turn left.\n",
    "3 - turn right.\n",
    "\n",
    "The task is episodic, and in order to solve the environment, the agent must get an average score of +13 over 100 consecutive episodes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "The model used as a function aproximator. The model has 3 linear layers, the firts 2 with a Relu activation function. The first layer with 37 input parameters and 64 output parameters. The second one with 64 input parameters and 64 input parameters. And the third one with 64 input parameters and 4 (the  posible actions). The optimization was made using ADAM optimizer to find the difference between the returned value and the expected one. \n",
    "\n",
    "This method is called Deep Q-learning and allows the agent to use the DNN to detect certain patterns and find an optimal way to solve the situations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent is built with the following architecture:\n",
    "<br/>\n",
    "A target DNN: This DNN returns the predicted value. This is the answer that the model finds correct at the moment of making a choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QNetwork(\n",
      "  (fc1): Linear(in_features=37, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(Agent(37,4,0).qnetwork_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A local DNN: This DNN has an architecture identical to the previous one. This DNN is in charge of removing the bias generated by the target network and avoid the inestability generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QNetwork(\n",
      "  (fc1): Linear(in_features=37, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(Agent(37,4,0).qnetwork_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A replay buffer:\n",
    "This is an implementation of experienced replay, allows the agent to collect the onformetion of the previous episodes. This buffer creates a custom real dataset from whcih the model can learn.\n",
    "<br/>\n",
    "# DQN algorithm\n",
    "The agent uses a Deep Q-learning algorithm (as stated before), this is a more complex implementation of the Q-learning algorithm, which updates it's values based on a guess on how the reward would be if the model follow the current path. But getting values from a guess might lead to uncertain correlations. Therefore we add a DNN to the equation, this allows the model to update the aproximation w parameters.\n",
    "\n",
    "\n",
    "<strong> $\\delta{w} = α ⋅ (R + \\gamma max\\hat{q}(S\\prime,α,w^{-}) - \\hat{q}(S,A,w))\\delta{w}\\hat{q}(S,A,w) $ </strong>\n",
    "\n",
    "The parameters are as follows:\n",
    "<br/>\n",
    "BUFFER_SIZE = int(1e5)  \n",
    "BATCH_SIZE = 64         \n",
    "GAMMA = 0.99            \n",
    "TAU = 1e-3              \n",
    "LR = 5e-4               \n",
    "UPDATE_EVERY = 4        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "The goal of the project was reached, get an average of +15 over 100 episodes.\n",
    "This is a deep reinforcement learning project.\n",
    "The models built here were created in order to solve one main problem, collect the most yellow bananas and avoid the blue ones. \n",
    "All techniques used in the development of the agent were implemented with the objetive to improve the performance and reach the convergence.\n",
    "Here is the training plot where we can see how much does the angent improves with the training process. The agent was able to solve the enviroment in 592 episodes, reaching an average reward of +15 over 100 episodes.\n",
    "\n",
    "<img src=\"trainingPlot.png\">\n",
    "\n",
    "All this process leads to future improvements and architectures to try. which are:\n",
    "<ul>\n",
    "    <li>Double DQN</li>\n",
    "    <li>Dueling DQN</li>\n",
    "    <li>Prioritized experience replay agent</li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
